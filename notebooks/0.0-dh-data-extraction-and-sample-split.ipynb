{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "This notebook includes two main parts:\n",
    "1. Load original datasets that are given in `pd.DataFrame` and save them in seperate `.npy` files, one for each %patient \\times lead%, in a way compatible with `DataLoader` class. \n",
    "2. Split patients into `train`, `validation` and `test` samples. It is done once for reproducibility. The current samples are 85%, 10% and 5% for `train`, `validation` and `test` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "\n",
    "# configurables\n",
    "root = '/home/david/Desktop/projects/thesis/upstream_seq2seq/'\n",
    "SOURCE_FOLDER = f'{root}/data/raw/'\n",
    "DESTINATION_FOLDER = f'{root}/data/processed/'\n",
    "train_ratio = 0.85\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.5\n",
    "CHUNKSIZE = 1000\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['georgia_combined_file.csv', 'china_combined_file.csv', 'ningbo_2_combined_file.csv', 'china_combined_file_targets.csv', 'chapman_combined_file.csv']\n",
      "Start Saving georgia_combined_file.csv (0/5)\n",
      "\tLoading Data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 125it [17:41,  8.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Saving china_combined_file.csv (1/5)\n",
      "\tLoading Data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 37it [05:12,  8.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Saving ningbo_2_combined_file.csv (2/5)\n",
      "\tLoading Data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 334it [49:51,  8.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Saving china_combined_file_targets.csv (3/5)\n",
      "\tLoading Data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 4it [00:00,  8.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Saving chapman_combined_file.csv (4/5)\n",
      "\tLoading Data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving files: 90it [14:03,  9.37s/it]\n"
     ]
    }
   ],
   "source": [
    "combined_files = [x for x in os.listdir(SOURCE_FOLDER) if 'combined_file' in x]\n",
    "combined_files.remove('ningbo_1_combined_file.csv')\n",
    "print(combined_files)\n",
    "\n",
    "for i, conbined_file in enumerate(combined_files):\n",
    "    print(f'Start Saving {conbined_file} ({i}/{len(combined_files)})')\n",
    "    print('\\tLoading Data..')\n",
    "\n",
    "    with pd.read_csv(SOURCE_FOLDER+conbined_file, chunksize=CHUNKSIZE, index_col=0) as reader:\n",
    "        for chunk in tqdm(reader, desc='Saving files'):\n",
    "            for j in range(len(chunk)):\n",
    "                signal, indx = chunk.iloc[j].values[:-1], chunk.iloc[j].values[-1]\n",
    "                with open(DESTINATION_FOLDER+f'{indx}.npy', 'wb') as f:\n",
    "                    np.save(f, signal)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ningbo_files 333072\n",
      "georgia_files 124128\n",
      "china_files 36984\n",
      "chapman_files 89256\n",
      "sum 583440\n"
     ]
    }
   ],
   "source": [
    "saved_files = os.listdir(DESTINATION_FOLDER)\n",
    "\n",
    "ningbo_files = [x for x in saved_files if 'ningbo' in x]\n",
    "georgia_files = [x for x in saved_files if 'georgia' in x]\n",
    "china_files = [x for x in saved_files if 'china' in x]\n",
    "chapman_files = [x for x in saved_files if 'chapman' in x]\n",
    "print('ningbo_files', len(ningbo_files))\n",
    "print('georgia_files', len(georgia_files))\n",
    "print('china_files', len(china_files))\n",
    "print('chapman_files', len(chapman_files))\n",
    "print('sum', len(ningbo_files)+len(georgia_files)+len(china_files)+len(chapman_files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "a = np.load(DESTINATION_FOLDER+f'{indx}.npy',allow_pickle=True)\n",
    "np.mean(a == chunk.iloc[j].values[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 0.0.npy from /home/david/Desktop/projects/thesis/upstream_seq2seq//data/processed/\n",
      "Removing 1.0.npy from /home/david/Desktop/projects/thesis/upstream_seq2seq//data/processed/\n"
     ]
    }
   ],
   "source": [
    "# removing two extra files\n",
    "for f in [x for x in saved_files if '0.0' in x or '1.0' in x]:\n",
    "    try:\n",
    "        print(f'Removing {f} from {DESTINATION_FOLDER}')\n",
    "        os.remove(DESTINATION_FOLDER+f'{f}')\n",
    "    except:\n",
    "        print(f'{f} already has been removed from {DESTINATION_FOLDER}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train-Validation-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Persons: 48623\n",
      "Firt 10 persons: ['georgia_E02735', 'georgia_E00265', 'china_Q0751', 'ningbo_JS27234', 'ningbo_JS22082', 'ningbo_JS15914', 'ningbo_JS25957', 'georgia_E05352', 'ningbo_JS16023', 'chapman_JS04397']\n",
      "Train indices No.: 41329\n",
      "Validation indices No.: 4862\n",
      "Test indices No.: 2432\n",
      "Train indices portion: 0.8499886884807601\n",
      "Validation indices portion: 0.09999383008041463\n",
      "Test indices portion: 0.05001748143882525\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(DESTINATION_FOLDER)\n",
    "[x for x in files if 'chapman' in x and 'JS02897' in x]\n",
    "datasource_and_person_level_index = {x.split('_')[0]+'_'+x.replace('.npy','').split('_')[-1] for x in files}\n",
    "print('Total Unique Persons:', len(datasource_and_person_level_index))\n",
    "print('Firt 10 persons:', list(datasource_and_person_level_index)[:10])\n",
    "\n",
    "datasource_and_person_level_index = list(datasource_and_person_level_index)\n",
    "#      set seed    shuffle the list\n",
    "random.Random(42).shuffle(datasource_and_person_level_index)\n",
    "\n",
    "N = len(datasource_and_person_level_index)\n",
    "train_indices = datasource_and_person_level_index[:int(train_ratio*N)]\n",
    "validation_indices = datasource_and_person_level_index[int(train_ratio*N):int((train_ratio+validation_ratio)*N)]\n",
    "test_indices = datasource_and_person_level_index[int((train_ratio+validation_ratio)*N):]\n",
    "\n",
    "print('Train indices No.:', len(train_indices))\n",
    "print('Validation indices No.:', len(validation_indices))\n",
    "print('Test indices No.:', len(test_indices))\n",
    "print('Train indices portion:', len(train_indices)/N)\n",
    "print('Validation indices portion:', len(validation_indices)/N)\n",
    "print('Test indices portion:', len(test_indices)/N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save in a dictionary\n",
    "data_splits = {'train':train_indices,\n",
    "               'validation':validation_indices,\n",
    "               'test':test_indices}\n",
    "\n",
    "# create a binary pickle file \n",
    "f = open(DESTINATION_FOLDER+\"splits.pkl\",\"wb\")\n",
    "\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(data_splits,f)\n",
    "\n",
    "# close file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open(DESTINATION_FOLDER+\"splits.pkl\", 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "\n",
    "print(data_splits == b)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splits\n",
      "Train:  []\n",
      "Val:  []\n",
      "Test:  []\n",
      "noise\n",
      "Train:  ['noise_1', 'noise_2']\n",
      "Val:  []\n",
      "Test:  []\n"
     ]
    }
   ],
   "source": [
    "with open(DESTINATION_FOLDER+\"splits.pkl\", 'rb') as handle:\n",
    "    splits = pickle.load(handle)\n",
    "print('splits')\n",
    "print('Train: ', [x for x in splits['train'] if 'split' in x])\n",
    "print('Val: ', [x for x in splits['validation'] if 'split' in x])\n",
    "print('Test: ', [x for x in splits['test'] if 'split' in x])\n",
    "\n",
    "\n",
    "print('noise')\n",
    "print('Train: ', [x for x in splits['train'] if 'noise' in x])\n",
    "print('Val: ', [x for x in splits['validation'] if 'noise' in x])\n",
    "print('Test: ', [x for x in splits['test'] if 'noise' in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits['train'].remove('splits.pkl_splits.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits['train'].remove('noise_1')\n",
    "splits['train'].remove('noise_2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:  []\n",
      "Val:  []\n",
      "Test:  []\n",
      "Train:  []\n",
      "Val:  []\n",
      "Test:  []\n"
     ]
    }
   ],
   "source": [
    "print('Train: ', [x for x in splits['train'] if 'split' in x])\n",
    "print('Val: ', [x for x in splits['validation'] if 'split' in x])\n",
    "print('Test: ', [x for x in splits['test'] if 'split' in x])\n",
    "print('Train: ', [x for x in splits['train'] if 'noise' in x])\n",
    "print('Val: ', [x for x in splits['validation'] if 'noise' in x])\n",
    "print('Test: ', [x for x in splits['test'] if 'noise' in x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resave\n",
    "\n",
    "# create a binary pickle file \n",
    "f = open(DESTINATION_FOLDER+\"splits.pkl\",\"wb\")\n",
    "# write the python object (dict) to pickle file\n",
    "pickle.dump(splits,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
