{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run modeling_THESIS.ipynb\n",
    "%run training_utils_THESIS.ipynb\n",
    "%run DataLoader_THESIS.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(seed, batch_size, data_folder_path, lead1, lead2,\n",
    "         input_dimension, output_dimension, \n",
    "         hidden_dimmension, attention_heads, \n",
    "         encoder_number_of_layers, decoder_number_of_layers, dropout,clip,\n",
    "         n_epochs, window, stride,init_token, n_iters,\n",
    "         # presentation\n",
    "         num_plots,\n",
    "         ):\n",
    "    \n",
    "    # Fix randomness\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    # Load splits dictionary\n",
    "    saved_files = os.listdir(data_folder_path)\n",
    "    with open(data_folder_path+\"splits.pkl\", 'rb') as handle:\n",
    "        splits = pickle.load(handle)\n",
    "\n",
    "    # Create data generators\n",
    "    train_generator = DataGenerator(lead1, lead2,                        # leads\n",
    "                                    data_folder_path,                    # path to relevant folder\n",
    "                                    batch_size,                          # batch size\n",
    "                                    list_IDs=splits['train'],            # list of relevat IDs (each ID is given in a <file>_<person> format)\n",
    "                                    shuffle = True                       # Whether to shuffle the list of IDs at the end of each epoch.\n",
    "                                    )\n",
    "    validation_generator = DataGenerator(lead1, lead2,                        # leads\n",
    "                                    data_folder_path,                    # path to relevant folder\n",
    "                                    batch_size,                          # batch size\n",
    "                                    list_IDs=splits['validation'],            # list of relevat IDs (each ID is given in a <file>_<person> format)\n",
    "                                    shuffle = True                       # Whether to shuffle the list of IDs at the end of each epoch.\n",
    "                                    )\n",
    "    test_generator = DataGenerator(lead1, lead2,                        # leads\n",
    "                                    data_folder_path,                    # path to relevant folder\n",
    "                                    batch_size,                          # batch size\n",
    "                                    list_IDs=splits['test'],            # list of relevat IDs (each ID is given in a <file>_<person> format)\n",
    "                                    shuffle = True                       # Whether to shuffle the list of IDs at the end of each epoch.\n",
    "                                    )\n",
    "    \n",
    "    \n",
    "    # create a model\n",
    "    model = TransformerModel(input_dimension, output_dimension, \n",
    "                        hidden_dimmension, attention_heads, \n",
    "                        encoder_number_of_layers, decoder_number_of_layers, dropout).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "    print(model)\n",
    "\n",
    "    train_and_validate(\n",
    "        # modeling\n",
    "        model, \n",
    "        optimizer, criterion, clip,\n",
    "        # data\n",
    "        train_generator, validation_generator,\n",
    "        # training\n",
    "        window, stride, init_token,device, n_epochs, n_iters, \n",
    "        # plot validation at the end of an epoch\n",
    "        num_plots,\n",
    "        test_samples = None,\n",
    "        initial_best_valid_loss = float('inf'))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 350,787 trainable parameters\n",
      "TransformerModel(\n",
      "  (encoder): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (pos_decoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=64, out_features=256, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Within epoch loss (training) 0.0126:  68%|██████▊   | 874/1291 [2:22:17<1:09:42, 10.03s/it] "
     ]
    }
   ],
   "source": [
    "    \n",
    "config = {\n",
    "    # general\n",
    "    'seed':123,\n",
    "    'data_folder_path': '../../dissertation/upstream_data/',\n",
    "    'lead1':\"LI\", 'lead2':\"LII\",\n",
    "    # training\n",
    "    'batch_size':32,\n",
    "    'n_epochs':10,\n",
    "    # architecture - to be changed later and pushed out towards tuning\n",
    "    'init_token': 0,\n",
    "    'input_dimension':1, 'output_dimension':1,  # these should remain constant\n",
    "    'hidden_dimmension': 64,                               # d_model (int) – the number of expected features in the input (required)???\n",
    "    'attention_heads':None,                         # number of attention heads, if None then d_model//64\n",
    "    'encoder_number_of_layers':3,\n",
    "    'decoder_number_of_layers':3,\n",
    "    'dropout':0.1,\n",
    "    'clip':1,\n",
    "    'window':500, \n",
    "    'stride':0.1,\n",
    "    'n_iters': None, # number of signals to train on. All if None\n",
    "    'num_plots':5,\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "main(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
